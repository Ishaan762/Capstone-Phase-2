{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9209631,"sourceType":"datasetVersion","datasetId":5568680}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-23T05:21:41.554332Z","iopub.execute_input":"2024-08-23T05:21:41.554739Z","iopub.status.idle":"2024-08-23T05:21:41.582429Z","shell.execute_reply.started":"2024-08-23T05:21:41.554701Z","shell.execute_reply":"2024-08-23T05:21:41.581273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf=pd.read_csv('/kaggle/input/20th-aug-images/updated_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-23T05:21:41.584205Z","iopub.execute_input":"2024-08-23T05:21:41.584566Z","iopub.status.idle":"2024-08-23T05:21:43.654248Z","shell.execute_reply.started":"2024-08-23T05:21:41.584530Z","shell.execute_reply":"2024-08-23T05:21:43.652768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-23T05:21:43.656353Z","iopub.execute_input":"2024-08-23T05:21:43.664301Z","iopub.status.idle":"2024-08-23T05:21:43.727276Z","shell.execute_reply.started":"2024-08-23T05:21:43.664219Z","shell.execute_reply":"2024-08-23T05:21:43.726292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_text_by_words(text):\n    words = text.split()\n    mid = len(words) // 2\n    \n    first_part = ' '.join(words[:mid])\n    second_part = ' '.join(words[mid:])\n    \n    return first_part, second_part\n\ndf[['Part1', 'second_part']] = df['text'].apply(lambda x: pd.Series(split_text_by_words(x)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['Split_Headline_Responses']=str(df['Split_Headline_Responses'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install sentence_transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom transformers import BertTokenizer, BertModel\nfrom sentence_transformers import SentenceTransformer\nimport torch\nfrom tqdm import tqdm\n\n# Load the USE model\nuse_embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\nbert_tokenizer = BertTokenizer.from_pretrained(model_name)\nbert_model = BertModel.from_pretrained(model_name)\n\n# Load Sentence-BERT model\nsbert_model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n\n# Function to get USE embedding\ndef get_use_embedding(texts):\n    return use_embed(texts).numpy()\n\n# Function to get BERT embedding\ndef get_bert_embedding(texts):\n    inputs = bert_tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = bert_model(**inputs)\n    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n\n# Function to get Sentence-BERT embedding\ndef get_sbert_embedding(texts):\n    return sbert_model.encode(texts)\n\ndef calculate_similarity(emb1, emb2):\n    return 1 - cosine(emb1, emb2)\n\n# Function to process data in batches\ndef process_data_in_batches(df, batch_size=32):\n    features = []\n    for i in tqdm(range(0, len(df), batch_size)):\n        batch = df.iloc[i:i+batch_size]\n        \n        use_emb1 = get_use_embedding(batch['second_part'].tolist())\n        use_emb2 = get_use_embedding(batch['Split_Headline_Responses'].tolist())\n        bert_emb1 = get_bert_embedding(batch['second_part'].tolist())\n        bert_emb2 = get_bert_embedding(batch['Split_Headline_Responses'].tolist())\n        sbert_emb1 = get_sbert_embedding(batch['second_part'].tolist())\n        sbert_emb2 = get_sbert_embedding(batch['Split_Headline_Responses'].tolist())\n        \n        use_similarities = np.array([calculate_similarity(e1, e2) for e1, e2 in zip(use_emb1, use_emb2)])\n        bert_similarities = np.array([calculate_similarity(e1, e2) for e1, e2 in zip(bert_emb1, bert_emb2)])\n        sbert_similarities = np.array([calculate_similarity(e1, e2) for e1, e2 in zip(sbert_emb1, sbert_emb2)])\n        \n        batch_features = np.concatenate([use_emb1, use_emb2, bert_emb1, bert_emb2, sbert_emb1, sbert_emb2,\n                                         use_similarities.reshape(-1, 1), \n                                         bert_similarities.reshape(-1, 1),\n                                         sbert_similarities.reshape(-1, 1)], axis=1)\n        features.append(batch_features)\n    \n    return np.concatenate(features, axis=0)\n\n# Process data in batches\nprint(\"Processing data in batches...\")\nX = process_data_in_batches(df)\ny = df['label'].values","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2=df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install salesforce-lavis","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['path'] = df['path'].str.replace('/content/drive/MyDrive/20th_aug_capstone','/kaggle/input/20th-aug-images')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device set to:\", device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from lavis.models import load_model_and_preprocess\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_feature_extractor\", model_type=\"base\", is_eval=True, device=device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\n\nmultimodal_embeddings = []\nimage_embeddings = []\ntext_embeddings = []\n\nfor index, row in df.iterrows():\n    image_path = row['path']\n    image = Image.open(image_path).convert(\"RGB\")\n    \n    text = row['text']\n    text_input = txt_processors[\"eval\"](text)\n    \n    image_processed = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n    sample = {\"image\": image_processed, \"text_input\": [text_input]}\n    \n    multimodal_emb = model.extract_features(sample).multimodal_embeds[0,0,:] \n    image_emb = model.extract_features(sample, mode=\"image\").image_embeds[0,0,:] \n    text_emb = model.extract_features(sample, mode=\"text\").text_embeds[0,0,:]\n    \n    multimodal_embeddings.append(multimodal_emb.cpu().numpy())\n    image_embeddings.append(image_emb.cpu().numpy())\n    text_embeddings.append(text_emb.cpu().numpy())\n\ndf['Multimodal Embeddings'] = multimodal_embeddings\ndf['Image Embeddings'] = image_embeddings\ndf['Text Embeddings'] = text_embeddings\nimport numpy as np\nmultimodal_embeddings =np.array(multimodal_embeddings)\ntext_embeddings = np.array(text_embeddings)\nimage_embeddings = np.array(image_embeddings)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score, classification_report\nimport numpy as np","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert similarity metrics features and labels to PyTorch tensors\nX_similarity = torch.tensor(X, dtype=torch.float32)\ny_similarity = torch.tensor(y, dtype=torch.float32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert multimodal inputs to tensors (assuming these are numpy arrays)\nmultimodal_inputs = torch.tensor(multimodal_embeddings, dtype=torch.float32)\nimage_inputs = torch.tensor(image_embeddings, dtype=torch.float32)\ntext_inputs = torch.tensor(text_embeddings, dtype=torch.float32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine all inputs for the BLIP embeddings\ncombined_blip_inputs = torch.cat((multimodal_inputs, image_inputs, text_inputs), dim=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Combine similarity metrics features and BLIP embeddings into one dataset\ncombined_inputs = torch.cat((combined_blip_inputs, X_similarity), dim=1)\nlabels_combined = torch.tensor(labels, dtype=torch.float32)  # Assuming the labels are the same for both parts","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train-test-validation split\nX_train, X_temp, y_train, y_temp = train_test_split(combined_inputs, labels_combined, test_size=0.3, random_state=2, stratify=labels_combined)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6667, random_state=2, stratify=y_temp)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create DataLoaders\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\ntest_dataset = TensorDataset(X_test, y_test)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the combined neural network for binary classification\nclass CombinedBinaryNet(nn.Module):\n    def __init__(self, blip_input_size, similarity_input_size):\n        super(CombinedBinaryNet, self).__init__()\n        combined_input_size = blip_input_size + similarity_input_size\n        self.fc1 = nn.Linear(combined_input_size, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, 1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = torch.sigmoid(self.fc3(x))\n        return x.squeeze()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the model\nblip_input_size = combined_blip_inputs.shape[1]\nsimilarity_input_size = X_similarity.shape[1]\nmodel = CombinedBinaryNet(blip_input_size, similarity_input_size)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Early stopping parameters\npatience = 10\nbest_val_loss = float('inf')\ncounter = 0","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\nnum_epochs = 100\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    model.train()\n    train_loss = 0.0\n    for inputs, targets in train_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item()\n    \n    train_loss /= len(train_loader)\n    val_loss /= len(val_loader)\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n    \n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        counter = 0\n        torch.save(model.state_dict(), 'best_combined_model.pth')\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f'Early stopping triggered after epoch {epoch+1}')\n            break","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load best model\nmodel.load_state_dict(torch.load('best_combined_model.pth'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate the model\nmodel.eval()\nall_preds = []\nall_targets = []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    for inputs, targets in test_loader:\n        inputs, targets = inputs.to(device), targets.to(device)\n        outputs = model(inputs)\n        predicted = (outputs > 0.5).float()\n        all_preds.extend(predicted.cpu().numpy())\n        all_targets.extend(targets.cpu().numpy())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert to numpy arrays\nall_preds = np.array(all_preds)\nall_targets = np.array(all_targets)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute and print accuracy\naccuracy = accuracy_score(all_targets, all_preds)\nprint(f'Test Accuracy: {accuracy:.4f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute and print F1 score\nf1 = f1_score(all_targets, all_preds)\nprint(f'Test F1 Score: {f1:.4f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(all_targets, all_preds))","metadata":{},"execution_count":null,"outputs":[]}]}