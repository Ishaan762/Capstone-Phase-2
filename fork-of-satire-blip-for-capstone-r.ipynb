{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9133337,"sourceType":"datasetVersion","datasetId":5514735}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install salesforce-lavis","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-10T18:29:01.343481Z","iopub.execute_input":"2024-08-10T18:29:01.343842Z","iopub.status.idle":"2024-08-10T18:30:11.147181Z","shell.execute_reply.started":"2024-08-10T18:29:01.343797Z","shell.execute_reply":"2024-08-10T18:30:11.146063Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting salesforce-lavis\n  Downloading salesforce_lavis-1.0.2-py3-none-any.whl.metadata (18 kB)\nCollecting contexttimer (from salesforce-lavis)\n  Downloading contexttimer-0.3.3.tar.gz (4.9 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting decord (from salesforce-lavis)\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nCollecting einops>=0.4.1 (from salesforce-lavis)\n  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\nCollecting fairscale==0.4.4 (from salesforce-lavis)\n  Downloading fairscale-0.4.4.tar.gz (235 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.4/235.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy (from salesforce-lavis)\n  Downloading ftfy-6.2.3-py3-none-any.whl.metadata (7.8 kB)\nCollecting iopath (from salesforce-lavis)\n  Downloading iopath-0.1.10.tar.gz (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ipython in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (8.20.0)\nCollecting omegaconf (from salesforce-lavis)\n  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\nCollecting opencv-python-headless==4.5.5.64 (from salesforce-lavis)\n  Downloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nCollecting opendatasets (from salesforce-lavis)\n  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (21.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (2.1.4)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (5.18.0)\nCollecting pre-commit (from salesforce-lavis)\n  Downloading pre_commit-3.8.0-py2.py3-none-any.whl.metadata (1.3 kB)\nCollecting pycocoevalcap (from salesforce-lavis)\n  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting pycocotools (from salesforce-lavis)\n  Downloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\nCollecting python-magic (from salesforce-lavis)\n  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (0.22.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (0.2.0)\nRequirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (3.7.3)\nCollecting streamlit (from salesforce-lavis)\n  Downloading streamlit-1.37.1-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting timm==0.4.12 (from salesforce-lavis)\n  Downloading timm-0.4.12-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (0.16.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (4.66.1)\nCollecting transformers<4.27,>=4.25.0 (from salesforce-lavis)\n  Downloading transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.3/100.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting webdataset (from salesforce-lavis)\n  Downloading webdataset-0.2.86-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from salesforce-lavis) (0.42.0)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python-headless==4.5.5.64->salesforce-lavis) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->salesforce-lavis) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->salesforce-lavis) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->salesforce-lavis) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->salesforce-lavis) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->salesforce-lavis) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->salesforce-lavis) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (0.22.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers<4.27,>=4.25.0->salesforce-lavis) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<4.27,>=4.25.0->salesforce-lavis)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->salesforce-lavis) (3.1.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->salesforce-lavis) (0.2.13)\nCollecting portalocker (from iopath->salesforce-lavis)\n  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (3.0.42)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (2.17.2)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (5.9.0)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython->salesforce-lavis) (4.8.0)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf->salesforce-lavis)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: kaggle in /opt/conda/lib/python3.10/site-packages (from opendatasets->salesforce-lavis) (1.6.12)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from opendatasets->salesforce-lavis) (8.1.7)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->salesforce-lavis) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->salesforce-lavis) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->salesforce-lavis) (2023.4)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->salesforce-lavis) (8.2.3)\nCollecting cfgv>=2.0.0 (from pre-commit->salesforce-lavis)\n  Downloading cfgv-3.4.0-py2.py3-none-any.whl.metadata (8.5 kB)\nCollecting identify>=1.0.0 (from pre-commit->salesforce-lavis)\n  Downloading identify-2.6.0-py2.py3-none-any.whl.metadata (4.4 kB)\nCollecting nodeenv>=0.11.1 (from pre-commit->salesforce-lavis)\n  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\nRequirement already satisfied: virtualenv>=20.10.0 in /opt/conda/lib/python3.10/site-packages (from pre-commit->salesforce-lavis) (20.21.0)\nRequirement already satisfied: matplotlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from pycocotools->salesforce-lavis) (3.7.5)\nRequirement already satisfied: scipy>=1.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image->salesforce-lavis) (1.11.4)\nRequirement already satisfied: pillow>=9.0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image->salesforce-lavis) (9.5.0)\nRequirement already satisfied: imageio>=2.27 in /opt/conda/lib/python3.10/site-packages (from scikit-image->salesforce-lavis) (2.33.1)\nRequirement already satisfied: tifffile>=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image->salesforce-lavis) (2023.12.9)\nRequirement already satisfied: lazy_loader>=0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-image->salesforce-lavis) (0.3)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (6.4.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (2.5.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (69.0.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->salesforce-lavis) (3.3.0)\nRequirement already satisfied: altair<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (5.3.0)\nRequirement already satisfied: blinker<2,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (1.7.0)\nRequirement already satisfied: cachetools<6,>=4.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (4.2.4)\nRequirement already satisfied: protobuf<6,>=3.20 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (3.20.3)\nRequirement already satisfied: pyarrow>=7.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (15.0.2)\nRequirement already satisfied: rich<14,>=10.14.0 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (13.7.0)\nRequirement already satisfied: toml<2,>=0.10.1 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (0.10.2)\nRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (3.1.41)\nCollecting pydeck<1,>=0.8.0b4 (from streamlit->salesforce-lavis)\n  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\nRequirement already satisfied: tornado<7,>=6.0.3 in /opt/conda/lib/python3.10/site-packages (from streamlit->salesforce-lavis) (6.3.3)\nCollecting watchdog<5,>=2.1.5 (from streamlit->salesforce-lavis)\n  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nCollecting braceexpand (from webdataset->salesforce-lavis)\n  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6,>=4.0->streamlit->salesforce-lavis) (0.12.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (4.0.11)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython->salesforce-lavis) (0.8.3)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.1.0->pycocotools->salesforce-lavis) (1.4.5)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython->salesforce-lavis) (0.7.0)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->salesforce-lavis) (2.14.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->salesforce-lavis) (2.1.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->salesforce-lavis) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<4.27,>=4.25.0->salesforce-lavis) (2024.2.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich<14,>=10.14.0->streamlit->salesforce-lavis) (3.0.0)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy->salesforce-lavis) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy->salesforce-lavis) (0.1.4)\nRequirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv>=20.10.0->pre-commit->salesforce-lavis) (0.3.8)\nCollecting platformdirs<4,>=2.4 (from virtualenv>=20.10.0->pre-commit->salesforce-lavis)\n  Downloading platformdirs-3.11.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy->salesforce-lavis) (0.16.0)\nRequirement already satisfied: python-slugify in /opt/conda/lib/python3.10/site-packages (from kaggle->opendatasets->salesforce-lavis) (8.0.4)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from kaggle->opendatasets->salesforce-lavis) (6.1.0)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->salesforce-lavis) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->salesforce-lavis) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython->salesforce-lavis) (0.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->salesforce-lavis) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->salesforce-lavis) (5.0.1)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->salesforce-lavis) (0.16.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit->salesforce-lavis) (0.1.2)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->kaggle->opendatasets->salesforce-lavis) (0.5.1)\nRequirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.10/site-packages (from python-slugify->kaggle->opendatasets->salesforce-lavis) (1.3)\nDownloading salesforce_lavis-1.0.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opencv_python_headless-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading timm-0.4.12-py3-none-any.whl (376 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.2.3-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\nDownloading pre_commit-3.8.0-py2.py3-none-any.whl (204 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.6/204.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pycocotools-2.0.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.8/427.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\nDownloading streamlit-1.37.1-py2.py3-none-any.whl (8.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading webdataset-0.2.86-py3-none-any.whl (70 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\nDownloading identify-2.6.0-py2.py3-none-any.whl (98 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\nDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\nDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\nDownloading platformdirs-3.11.0-py3-none-any.whl (17 kB)\nBuilding wheels for collected packages: fairscale, contexttimer, iopath, antlr4-python3-runtime\n  Building wheel for fairscale (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for fairscale: filename=fairscale-0.4.4-py3-none-any.whl size=292832 sha256=da55e6ef0d32256601177541cd47fc5076fa70d5b0b01baa2913bbf6f962ce02\n  Stored in directory: /root/.cache/pip/wheels/08/58/6f/56c57fa8315eb0bcf0287b580c850845be5f116359b809e9f1\n  Building wheel for contexttimer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for contexttimer: filename=contexttimer-0.3.3-py3-none-any.whl size=5804 sha256=b79633589fdb055f5a0af715f682d9323f62ab21824b505cc75756b4ab2ebe8b\n  Stored in directory: /root/.cache/pip/wheels/72/1c/da/cfd97201d88ccce214427fa84a5caeb91fef7c5a1b4c4312b4\n  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=0bbb49881eb772e9afe0280c1298f991f8af1de10971eedc17a54485085b435c\n  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=f7d2d5702bcadc13a690c6f8eae3cdae546fcd61407ddcd2f2fbe8c2098579fd\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\nSuccessfully built fairscale contexttimer iopath antlr4-python3-runtime\nInstalling collected packages: tokenizers, contexttimer, braceexpand, antlr4-python3-runtime, webdataset, watchdog, python-magic, portalocker, platformdirs, opencv-python-headless, omegaconf, nodeenv, identify, ftfy, einops, decord, cfgv, pydeck, iopath, transformers, pycocotools, pre-commit, opendatasets, fairscale, timm, pycocoevalcap, streamlit, salesforce-lavis\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\n  Attempting uninstall: platformdirs\n    Found existing installation: platformdirs 4.2.0\n    Uninstalling platformdirs-4.2.0:\n      Successfully uninstalled platformdirs-4.2.0\n  Attempting uninstall: opencv-python-headless\n    Found existing installation: opencv-python-headless 4.9.0.80\n    Uninstalling opencv-python-headless-4.9.0.80:\n      Successfully uninstalled opencv-python-headless-4.9.0.80\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.39.3\n    Uninstalling transformers-4.39.3:\n      Successfully uninstalled transformers-4.39.3\n  Attempting uninstall: timm\n    Found existing installation: timm 0.9.16\n    Uninstalling timm-0.9.16:\n      Successfully uninstalled timm-0.9.16\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab 4.1.6 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.26.1 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 braceexpand-0.1.7 cfgv-3.4.0 contexttimer-0.3.3 decord-0.6.0 einops-0.8.0 fairscale-0.4.4 ftfy-6.2.3 identify-2.6.0 iopath-0.1.10 nodeenv-1.9.1 omegaconf-2.3.0 opencv-python-headless-4.5.5.64 opendatasets-0.1.22 platformdirs-3.11.0 portalocker-2.10.1 pre-commit-3.8.0 pycocoevalcap-1.2 pycocotools-2.0.8 pydeck-0.9.1 python-magic-0.4.27 salesforce-lavis-1.0.2 streamlit-1.37.1 timm-0.4.12 tokenizers-0.13.3 transformers-4.26.1 watchdog-4.0.1 webdataset-0.2.86\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:30:11.148900Z","iopub.execute_input":"2024-08-10T18:30:11.149191Z","iopub.status.idle":"2024-08-10T18:30:11.153776Z","shell.execute_reply.started":"2024-08-10T18:30:11.149164Z","shell.execute_reply":"2024-08-10T18:30:11.152816Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(\"Device set to:\", device)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:30:11.154969Z","iopub.execute_input":"2024-08-10T18:30:11.155314Z","iopub.status.idle":"2024-08-10T18:30:14.323596Z","shell.execute_reply.started":"2024-08-10T18:30:11.155283Z","shell.execute_reply":"2024-08-10T18:30:14.322659Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Device set to: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"from lavis.models import load_model_and_preprocess\nmodel, vis_processors, txt_processors = load_model_and_preprocess(name=\"blip_feature_extractor\", model_type=\"base\", is_eval=True, device=device)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:30:14.327106Z","iopub.execute_input":"2024-08-10T18:30:14.327537Z","iopub.status.idle":"2024-08-10T18:31:37.602596Z","shell.execute_reply.started":"2024-08-10T18:30:14.327504Z","shell.execute_reply":"2024-08-10T18:31:37.601662Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-08-10 18:30:16.993821: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-10 18:30:16.993928: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-10 18:30:17.117228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd5482c32f904adb8e306854f53f2a7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ede6d377b1e4ad288665d0c0bbd78eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fea81a542a94753a2ba3ac1d44c5ccf"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1.97G/1.97G [00:57<00:00, 36.9MB/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/7th-aug-images/updated_dataset.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:31:37.603872Z","iopub.execute_input":"2024-08-10T18:31:37.604593Z","iopub.status.idle":"2024-08-10T18:31:37.744332Z","shell.execute_reply.started":"2024-08-10T18:31:37.604567Z","shell.execute_reply":"2024-08-10T18:31:37.743464Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['path'] = df['path'].str.replace('7th_aug_images','7th-aug-images')","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:31:37.745408Z","iopub.execute_input":"2024-08-10T18:31:37.745734Z","iopub.status.idle":"2024-08-10T18:31:37.767078Z","shell.execute_reply.started":"2024-08-10T18:31:37.745708Z","shell.execute_reply":"2024-08-10T18:31:37.766197Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:31:37.768368Z","iopub.execute_input":"2024-08-10T18:31:37.768839Z","iopub.status.idle":"2024-08-10T18:31:37.787172Z","shell.execute_reply.started":"2024-08-10T18:31:37.768812Z","shell.execute_reply":"2024-08-10T18:31:37.786261Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   Unnamed: 0                                               text  \\\n0           0    craigslist ad get paid to attend harvard for me   \n1           1                    this cat chillin on an arm rest   \n2           2  Disney Edits Controversial Mandalorian Scene S...   \n3           3  Hero: This Man Refused To Turn To His Neighbor...   \n4           4  bentley says national leaders should look to a...   \n\n                                           image_url  label  \\\n0  https://external-preview.redd.it/jt6tIxka4W2mj...      0   \n1  https://preview.redd.it/ygvtnvtoqb831.jpg?widt...      0   \n2  https://media.babylonbee.com/articles/article-...      1   \n3  https://media.babylonbee.com/articles/article-...      1   \n4  https://external-preview.redd.it/cpqJ9WmQhmFRA...      0   \n\n                                              path  \n0  /kaggle/input/7th-aug-images/images/image_0.jpg  \n1  /kaggle/input/7th-aug-images/images/image_1.jpg  \n2  /kaggle/input/7th-aug-images/images/image_2.jpg  \n3  /kaggle/input/7th-aug-images/images/image_3.jpg  \n4  /kaggle/input/7th-aug-images/images/image_4.jpg  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>text</th>\n      <th>image_url</th>\n      <th>label</th>\n      <th>path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>craigslist ad get paid to attend harvard for me</td>\n      <td>https://external-preview.redd.it/jt6tIxka4W2mj...</td>\n      <td>0</td>\n      <td>/kaggle/input/7th-aug-images/images/image_0.jpg</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>this cat chillin on an arm rest</td>\n      <td>https://preview.redd.it/ygvtnvtoqb831.jpg?widt...</td>\n      <td>0</td>\n      <td>/kaggle/input/7th-aug-images/images/image_1.jpg</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Disney Edits Controversial Mandalorian Scene S...</td>\n      <td>https://media.babylonbee.com/articles/article-...</td>\n      <td>1</td>\n      <td>/kaggle/input/7th-aug-images/images/image_2.jpg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Hero: This Man Refused To Turn To His Neighbor...</td>\n      <td>https://media.babylonbee.com/articles/article-...</td>\n      <td>1</td>\n      <td>/kaggle/input/7th-aug-images/images/image_3.jpg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>bentley says national leaders should look to a...</td>\n      <td>https://external-preview.redd.it/cpqJ9WmQhmFRA...</td>\n      <td>0</td>\n      <td>/kaggle/input/7th-aug-images/images/image_4.jpg</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:31:37.788199Z","iopub.execute_input":"2024-08-10T18:31:37.788471Z","iopub.status.idle":"2024-08-10T18:31:37.794142Z","shell.execute_reply.started":"2024-08-10T18:31:37.788448Z","shell.execute_reply":"2024-08-10T18:31:37.793289Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"21885"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport pandas as pd\n\nmultimodal_embeddings = []\nimage_embeddings = []\ntext_embeddings = []\n\nfor index, row in df.iterrows():\n    image_path = row['path']\n    image = Image.open(image_path).convert(\"RGB\")\n    \n    text = row['text']\n    text_input = txt_processors[\"eval\"](text)\n    \n    image_processed = vis_processors[\"eval\"](image).unsqueeze(0).to(device)\n    sample = {\"image\": image_processed, \"text_input\": [text_input]}\n    \n    multimodal_emb = model.extract_features(sample).multimodal_embeds[0,0,:] \n    image_emb = model.extract_features(sample, mode=\"image\").image_embeds[0,0,:] \n    text_emb = model.extract_features(sample, mode=\"text\").text_embeds[0,0,:]\n    \n    multimodal_embeddings.append(multimodal_emb.cpu().numpy())\n    image_embeddings.append(image_emb.cpu().numpy())\n    text_embeddings.append(text_emb.cpu().numpy())\n\ndf['Multimodal Embeddings'] = multimodal_embeddings\ndf['Image Embeddings'] = image_embeddings\ndf['Text Embeddings'] = text_embeddings\n","metadata":{"execution":{"iopub.status.busy":"2024-08-10T18:31:37.795215Z","iopub.execute_input":"2024-08-10T18:31:37.795496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multimodal_embeddings =np.array(multimodal_embeddings)\ntext_embeddings = np.array(text_embeddings)\nimage_embeddings = np.array(image_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multimodal_inputs = multimodal_embeddings  \nimage_inputs = image_embeddings       \ntext_inputs = text_embeddings        \nlabels = np.array(df['label'])             ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_multimodal, test_multimodal, train_image, test_image, train_text, test_text, train_labels, test_labels = train_test_split(\n    multimodal_inputs, image_inputs, text_inputs, labels, test_size=0.2, random_state=42, stratify=labels)\n\nval_multimodal, test_multimodal, val_image, test_image, val_text, test_text, val_labels, test_labels = train_test_split(\n    test_multimodal, test_image, test_text, test_labels, test_size=0.5, random_state=42, stratify=test_labels)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\nX_multimodal_train, X_multimodal_val, X_image_train, X_image_val = train_multimodal, val_multimodal,train_image,val_image \ny_train=torch.tensor(train_labels)\ny_val=torch.tensor(val_labels)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyModel1(nn.Module):\n    def __init__(self, multimodal_dim, image_dim, output_dim):\n        super(MyModel1, self).__init__()\n        self.fc1 = nn.Linear(multimodal_dim + image_dim, 1024)  \n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.Linear(256, 128)\n        self.fc5 = nn.Linear(128, 64)\n        self.fc6 = nn.Linear(64, 32)\n        self.fc7 = nn.Linear(32, 16)\n        self.fc8 = nn.Linear(16, 8)\n        self.fc9 = nn.Linear(8, 4)  \n        self.fc10 = nn.Linear(4, output_dim)  \n        self.relu = nn.ReLU()\n        self.output_dim = output_dim\n    \n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim=1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        x = self.relu(self.fc4(x))\n        x = self.relu(self.fc5(x))\n        x = self.relu(self.fc6(x))\n        x = self.relu(self.fc7(x))\n        x = self.relu(self.fc8(x))\n        x = self.relu(self.fc9(x))  \n        x = self.fc10(x)\n        return x\n\n\n\n\noutput_dim = 2\nmodel = MyModel1(multimodal_dim=multimodal_embeddings.shape[1], \n                image_dim=image_embeddings.shape[1], \n                output_dim=output_dim)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 200\nbest_loss = float('inf')\nearly_stopping_counter = 0\npatience = 5 \nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(torch.tensor(X_multimodal_train, dtype=torch.float), \n                    torch.tensor(X_image_train, dtype=torch.float))\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(torch.tensor(X_multimodal_val, dtype=torch.float), \n                        torch.tensor(X_image_val, dtype=torch.float))\n        val_loss = criterion(outputs, y_val)\n        _, predicted = torch.max(outputs, 1)\n        accuracy = accuracy_score(y_val, predicted)\n        print(f'Validation Loss: {val_loss}')\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(\"Early stopping!\")\n                break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_im.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX_text_train, X_text_val= train_text, test_text\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyModel2(nn.Module):\n    def __init__(self, multimodal_dim, text_dim, output_dim):\n        super(MyModel2, self).__init__()\n        self.fc1 = nn.Linear(multimodal_dim + text_dim, 1024)  \n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.Linear(256, 128)\n        self.fc5 = nn.Linear(128, 64)\n        self.fc6 = nn.Linear(64, 32)\n        self.fc7 = nn.Linear(32, 16)\n        self.fc8 = nn.Linear(16, 8)\n        self.fc9 = nn.Linear(8, 4)  \n        self.fc10 = nn.Linear(4, output_dim)  \n        self.relu = nn.ReLU()\n        self.output_dim = output_dim\n    \n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim=1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        x = self.relu(self.fc4(x))\n        x = self.relu(self.fc5(x))\n        x = self.relu(self.fc6(x))\n        x = self.relu(self.fc7(x))\n        x = self.relu(self.fc8(x))\n        x = self.relu(self.fc9(x))  \n        x = self.fc10(x)\n        return x\n\n# After train_test_split, check shapes\nprint(\"Train Multimodal Shape:\", X_multimodal_train.shape)\nprint(\"Train Text Shape:\", X_text_train.shape)\nprint(\"Val Multimodal Shape:\", X_multimodal_val.shape)\nprint(\"Val Text Shape:\", X_text_val.shape)\n\n# Verify that the sizes match before concatenation\nif X_multimodal_val.shape[0] != X_text_val.shape[0]:\n    print(f\"Shape mismatch: Multimodal val {X_multimodal_val.shape[0]} vs Text val {X_text_val.shape[0]}\")\n    # Optionally, slice the tensors to match sizes if the difference is small\n    min_size = min(X_multimodal_val.shape[0], X_text_val.shape[0])\n    X_multimodal_val = X_multimodal_val[:min_size]\n    X_text_val = X_text_val[:min_size]\n    y_val = y_val[:min_size]\n\n# Concatenate and continue with training\nX_val = torch.cat((torch.tensor(X_multimodal_val, dtype=torch.float), \n                   torch.tensor(X_text_val, dtype=torch.float)), dim=1)\n\noutput_dim = 2\nmodel = MyModel2(multimodal_dim=multimodal_embeddings.shape[1], \n                text_dim=text_embeddings.shape[1], \n                output_dim=output_dim)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 200\nbest_loss = float('inf')\nearly_stopping_counter = 0\npatience = 5  \nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(torch.tensor(X_multimodal_train, dtype=torch.float), \n                    torch.tensor(X_text_train, dtype=torch.float))\n    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.long))\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(torch.tensor(X_multimodal_val, dtype=torch.float), \n                        torch.tensor(X_text_val, dtype=torch.float))\n        val_loss = criterion(outputs, torch.tensor(y_val, dtype=torch.long))\n        _, predicted = torch.max(outputs, 1)\n        accuracy = accuracy_score(y_val, predicted)\n        print(f'Validation Loss: {val_loss}')\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(\"Early stopping!\")\n                break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_tm.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MyModel3(nn.Module):\n    def __init__(self, image_dim, text_dim, output_dim):\n        super(MyModel3, self).__init__()\n        self.fc1 = nn.Linear(image_dim + text_dim, 1024)  \n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.Linear(256, 128)\n        self.fc5 = nn.Linear(128, 64)\n        self.fc6 = nn.Linear(64, 32)\n        self.fc7 = nn.Linear(32, 16)\n        self.fc8 = nn.Linear(16, 8)\n        self.fc9 = nn.Linear(8, 4)  \n        self.fc10 = nn.Linear(4, output_dim)  \n        self.relu = nn.ReLU()\n        self.output_dim = output_dim\n    \n    def forward(self, x1, x2):\n        x = torch.cat((x1, x2), dim=1)\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.relu(self.fc3(x))\n        x = self.relu(self.fc4(x))\n        x = self.relu(self.fc5(x))\n        x = self.relu(self.fc6(x))\n        x = self.relu(self.fc7(x))\n        x = self.relu(self.fc8(x))\n        x = self.relu(self.fc9(x))  \n        x = self.fc10(x)\n        return x\n\n\n\n\noutput_dim = 2\nmodel = MyModel3(image_dim=image_embeddings.shape[1], \n                text_dim=text_embeddings.shape[1], \n                output_dim=output_dim)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nnum_epochs = 200\nbest_loss = float('inf')\nearly_stopping_counter = 0\npatience = 5  \nfor epoch in range(num_epochs):\n    model.train()\n    optimizer.zero_grad()\n    outputs = model(torch.tensor(X_image_train, dtype=torch.float), \n                    torch.tensor(X_text_train, dtype=torch.float))\n    loss = criterion(outputs, torch.tensor(y_train, dtype=torch.long))\n    loss.backward()\n    optimizer.step()\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n    \n    model.eval()\n    with torch.no_grad():\n        outputs = model(torch.tensor(X_image_val, dtype=torch.float), \n                        torch.tensor(X_text_val, dtype=torch.float))\n        val_loss = criterion(outputs, torch.tensor(y_val, dtype=torch.long))\n        _, predicted = torch.max(outputs, 1)\n        accuracy = accuracy_score(y_val, predicted)\n        print(f'Validation Loss: {val_loss}')\n        \n        if val_loss < best_loss:\n            best_loss = val_loss\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(\"Early stopping!\")\n                break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'model_it.pth')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = MyModel1(multimodal_dim=multimodal_embeddings.shape[1], \n                image_dim=image_embeddings.shape[1], \n                output_dim=output_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model3 = MyModel3(text_dim=text_embeddings.shape[1], \n                image_dim=image_embeddings.shape[1], \n                output_dim=output_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2 = MyModel2(multimodal_dim=multimodal_embeddings.shape[1], \n                text_dim=text_embeddings.shape[1], \n                output_dim=output_dim)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1.load_state_dict(torch.load('/kaggle/working/model_im.pth'))\nmodel2.load_state_dict(torch.load('/kaggle/working/model_it.pth'))\nmodel3.load_state_dict(torch.load('/kaggle/working/model_tm.pth'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FusionModel(nn.Module):\n    def __init__(self, model1, model2, model3, output_dim):\n        super(FusionModel, self).__init__()\n        self.model1 = model1\n        self.model2 = model2\n        self.model3 = model3\n        \n        self.fc1 = nn.Linear(in_features=model1.output_dim + model2.output_dim + model3.output_dim, out_features=128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 32)\n        self.fc4 = nn.Linear(32, output_dim) \n        self.relu = nn.ReLU()\n    \n    def forward(self, multimodal_input, image_input, text_input):\n        multimodal_output = self.model1(multimodal_input, image_input)\n        image_output = self.model3(image_input, text_input)\n        text_output = self.model2(multimodal_input, text_input)\n        fused_output = torch.cat((multimodal_output, image_output, text_output), dim=1)\n        fused_output = self.relu(self.fc1(fused_output))\n        fused_output = self.relu(self.fc2(fused_output))\n        fused_output = self.relu(self.fc3(fused_output))\n        final_output = self.fc4(fused_output)\n        return final_output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, multimodal_inputs, image_inputs, text_inputs, labels):\n        self.multimodal_inputs = multimodal_inputs\n        self.image_inputs = image_inputs\n        self.text_inputs = text_inputs\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, index):\n        multimodal_input = torch.tensor(self.multimodal_inputs[index], dtype=torch.float)\n        image_input = torch.tensor(self.image_inputs[index], dtype=torch.float)\n        text_input = torch.tensor(self.text_inputs[index], dtype=torch.float)\n        label = torch.tensor(self.labels[index], dtype=torch.long)\n        return multimodal_input, image_input, text_input, label\n\ntrain_dataset = CustomDataset(train_multimodal, train_image, train_text, train_labels)\nval_dataset = CustomDataset(val_multimodal, val_image, val_text, val_labels)\ntest_dataset = CustomDataset(test_multimodal, test_image, test_text, test_labels)\n\nbatch_size = 32\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs=10, patience=5):\n    model.to(device)\n    model.train()\n    best_val_loss = float('inf')\n    early_stopping_counter = 0\n    f1_score = F1Score(task=\"binary\", num_classes=2, average='macro').to(device)  # Assuming binary classification\n\n    \n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct_predictions = 0\n        total_samples = 0\n        for multimodal_input, image_input, text_input, labels in train_dataloader:\n            multimodal_input = multimodal_input.to(device)\n            image_input = image_input.to(device)\n            text_input = text_input.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(multimodal_input, image_input, text_input)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item() * labels.size(0)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n\n        epoch_loss = running_loss / total_samples\n        epoch_accuracy = correct_predictions / total_samples\n\n        model.eval()\n        val_correct_predictions = 0\n        val_total_samples = 0\n        val_running_loss = 0.0\n        with torch.no_grad():\n            for multimodal_input, image_input, text_input, labels in val_dataloader:\n                multimodal_input = multimodal_input.to(device)\n                image_input = image_input.to(device)\n                text_input = text_input.to(device)\n                labels = labels.to(device)\n                outputs = model(multimodal_input, image_input, text_input)\n                loss = criterion(outputs, labels)\n                val_running_loss += loss.item() * labels.size(0)\n                _, predicted = torch.max(outputs, 1)\n                val_correct_predictions += (predicted == labels).sum().item()\n                val_total_samples += labels.size(0)\n        val_loss = val_running_loss / val_total_samples\n        val_accuracy = val_correct_predictions / val_total_samples\n        \n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss:.4f}')\n        \n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            early_stopping_counter = 0\n        else:\n            early_stopping_counter += 1\n            if early_stopping_counter >= patience:\n                print(\"Early stopping!\")\n                break\n            \n    print('Training finished.')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nfusion_model = FusionModel(model1, model2, model3, output_dim=2)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(fusion_model.parameters(), lr=0.001)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchmetrics import F1Score\n\ndef test_model(model, test_dataloader, device):\n    model.eval()\n    correct_predictions = 0\n    total_samples = 0\n    f1_score = F1Score(task=\"binary\", num_classes=2, average='macro').to(device)\n\n    with torch.no_grad():\n        for multimodal_input, image_input, text_input, labels in test_dataloader:\n            multimodal_input = multimodal_input.to(device)\n            image_input = image_input.to(device)\n            text_input = text_input.to(device)\n            labels = labels.to(device)\n\n            outputs = model(multimodal_input, image_input, text_input)\n            _, predicted = torch.max(outputs, 1)\n            correct_predictions += (predicted == labels).sum().item()\n            total_samples += labels.size(0)\n            f1_score.update(predicted, labels)  # Update the F1Score metric\n\n    test_accuracy = correct_predictions / total_samples\n    test_f1_score = f1_score.compute()  # Compute the F1Score for the test set\n\n    print(f'Test Accuracy: {test_accuracy:.4f}, Test F1-Score: {test_f1_score:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntrain_model(fusion_model, train_dataloader, val_dataloader, criterion, optimizer, device, num_epochs=50)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_model(fusion_model, test_dataloader, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}